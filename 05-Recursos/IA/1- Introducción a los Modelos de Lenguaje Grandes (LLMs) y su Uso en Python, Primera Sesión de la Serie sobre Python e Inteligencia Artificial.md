
# Python + IA (Espa√±ol) (Microsoft Reactor)

Gwyneth Pe√±a-Siguenza 
Python Cloud Advocate
madebygps.com/about 
@madebygps
Alias (GPS) üòä

### C√≥mo Funcionan los LLMs

Empezando: **¬øC√≥mo funcionan los LLMs?** Una cosa que me encanta del espa√±ol, pero que a la vez llega a ser un poquito confuso cuando estoy creando contenido, es que hay un mont√≥n de maneras de decir una palabra. Por ejemplo, yo en Ecuador s√© que el "canguil" es popcorn, pero s√© que la gente lo dice "palomitas" o no s√©, pongan en el chat c√≥mo dicen "canguil" en su pa√≠s. Es lo mismo con este concepto de LLMs:  

- Grandes modelos de lenguaje.  
- Modelos de gran contexto del lenguaje.  
- Modelos de lenguaje grande.  

Hay un mont√≥n de opciones ah√≠, pero yo los voy a llamar **LLMs** de ah√≠ en adelante, ¬øsaben a lo que me refiero, no? Lo genial es que a lo mejor ustedes ya han utilizado un LLM, bien sea en algo como **chatgpt.com** o si han utilizado **Copilot**. ¬øQu√© tal les parece Copilot? M√≠renlo en el chat. ¬øHan utilizado Copilot? ¬øCosas como Claude, Gemini? Bueno, un mont√≥n de cosas ah√≠, a lo mejor ustedes ya han utilizado, ¬øno es cierto? Es importante saber que la tecnolog√≠a detr√°s de esas aplicaciones es el modelo en s√≠, pero hay funcionalidad que estos programas tienen que no necesariamente son parte del LLM.

**¬øQu√© es un modelo de lenguaje grande?** Es un modelo de **machine learning**, que es un espacio que ya existe por muchos a√±os. No es que ChatGPT empez√≥ y pues ah√≠ empez√≥ esto de inteligencia artificial y machine learning. No, ya lleva d√©cadas siendo una cosa. Un LLM no es nada m√°s que un modelo de machine learning que es tan, pero tan, pero tan grande, que logra entender, comprender y generar lenguaje de uso cotidiano, de manera que nosotros hablamos. Por ejemplo: "Hola, ¬øqu√© tal?" El modelo tambi√©n puede decir: "Todo bien, ¬øy t√∫?" Algo as√≠, ¬øno?

# Ejemplos

1. Tenemos alguna **entrada** (input) en lenguaje natural, que puede ser una oraci√≥n, un p√°rrafo, lo que sea.  
2. Estos modelos entienden a trav√©s de algo que se llama un **token**. Como nosotros los humanos entendemos en palabras, algo equivalente ser√≠a un token para el modelo. Eso llegar√≠a a ser una **representaci√≥n num√©rica de la palabra**, porque las computadoras entienden n√∫meros, ¬øno es cierto?  
3. Esos tokens se pasan al modelo, y el modelo utiliza esto para devolver la palabra que tiene m√°s sentido o m√°s relaci√≥n con la palabra anterior o al contexto anterior.  
4. Despu√©s, **decodifica** eso (porque obviamente esto estar√≠a en representaci√≥n num√©rica), lo post-procesa para crear palabras a trav√©s de esos n√∫meros y devuelve el resultado a ustedes.  

Por ejemplo, cuando ponen "Hola, ¬øc√≥mo est√°s?", el modelo les dice "Bien, ¬øy t√∫?" o algo as√≠. Todo eso que va en la mitad son n√∫meros, ¬øno es cierto? Vemos que, por ejemplo, la entrada "Hablaremos de LLMs" llegar√≠a a ser 1, 2, 3, 4, 5 tokens. En ingl√©s es casi una palabra por token, m√°s o menos por ah√≠, y para otros lenguajes, en este caso espa√±ol, vemos que tres palabras se convirtieron en seis tokens. Esto es porque la mayor√≠a de estos modelos, la cantidad de informaci√≥n con la que se entrenan, est√° en ingl√©s, por lo que les cuesta m√°s computaci√≥n, m√°s tokens y todo eso entrenarlos en otro lenguaje.

![[1.1- Tokenizacion.png]]

Piensen que el modelo es como un **diccionario**. Este token tiene el ID 39, entonces el modelo sabe: "Oh, tengo que ir a la identificaci√≥n 39 para encontrar este token". Nuestra entrada se convierte en estos tokens (en este caso ser√≠an seis), y el modelo nos devuelve el token que piensa que tiene que ser el siguiente seg√∫n el contexto. El contexto es todo lo que sabe, ¬øno es cierto? Al comienzo, es simplemente "Hablaremos de los LLMs", y despu√©s nos devuelve "hoy". Luego, el contexto crece: "Hablaremos de los LLMs hoy", y sigue creciendo hasta terminar y nos devuelve algo como: "Hablaremos de los LLMs hoy en la clase de programaci√≥n". Esto quiere decir que el trabajo del modelo es devolver la siguiente palabra o token que tiene m√°s sentido, calculado a trav√©s de una probabilidad.

![[1.2- Contexto.png]]

Una manera sencilla de describir c√≥mo trabajan estos LLMs es decir que simplemente **predicen el siguiente token**. Es como un **autocompletar**, pero muy, muy, muy sofisticado. De seguro han escrito un mensaje de texto, ponen "Hola, ¬øc√≥mo est√°s?" y les sale una palabrita en gris que pueden darle clic y les completa todo. Eso es casi como el trabajo de un modelo de este tipo, solo que son muy, muy grandes, tienen un entendimiento muy grande y mucha composici√≥n detr√°s de ellos. Pero, en general, en resumen, as√≠ es como trabajan, ¬øno?

![[1.3- Flujo de LLM.png]]

---

### Opciones de Modelos

Hablemos de **opciones de modelos**:  
1. **Gestionados (Hosted):** Estos est√°n en la infraestructura de otra compa√±√≠a.  
   - Ejemplos:  
     - **OpenAI** (openai.com): Modelos como GPT-3.5 (que casi ya no se usa, se considera legacy), GPT-4, GPT-4o (lo que se utiliza m√°s ahora).  
     - **Azure AI**: Hay un mont√≥n, OpenAI GPT models, modelos de Meta, modelos de Cohere, Mistral, DeepSeek.
     - **GitHub Models**: Vamos a trabajar much√≠simo con GitHub Models porque tiene un mont√≥n de opciones, una selecci√≥n del cat√°logo de modelos que tiene Azure, y es **gratis**. Solo necesitan una cuenta de GitHub, nada m√°s. Por eso, cuando hacemos esta serie, los demos, los ejercicios, nos gusta utilizar lo que es gratis y todos van a tener acceso a eso.  
     - **Google**: Gemini, Claude.
   - Si van a este enlace [Models ¬∑ GitHub Marketplace](https://github.com/marketplace/models), les da una interfaz donde pueden seleccionar los modelos y trabajar. Por ejemplo, un ejemplo con OpenAI GPT-4o Mini. Obviamente tienen limitaciones grandes y no van a poder desplegar aplicaciones que deban estar en producci√≥n o tener escala de producci√≥n ah√≠, pero para experimentar y probar, muy genial.

2. **Modelos Locales:**  
   - Tienen **open weights** y pueden funcionar en m√°quinas personales o las m√°quinas que tengan ustedes.  
   - Tambi√©n hay algo que se conoce como **SLMs (Small Language Models)**. No hay una descripci√≥n concreta, pero m√°s o menos se podr√≠a decir que son modelos m√°s peque√±os, menores a 100 billones de par√°metros. Lo dif√≠cil de distinguir aqu√≠ es que, por ejemplo, la m√°quina que tengo yo puede correr ciertos modelos, y a lo mejor t√∫ tienes una m√°quina mucho m√°s potente y hay modelos que t√∫ s√≠ puedes correr y yo no. Pero, por lo general, los SLMs los podemos definir as√≠.  
   - Cuando quieres trabajar de manera local, tienes que utilizar algo que se llama un **local model runner**. Los m√°s populares ser√≠an:  
     - **Ollama**.  
     - **LLaMA File**.  
     - **LM Studio**.  
   - Ejemplo con Ollama: Si abro mi terminal y pongo `ollama run` y el nombre del modelo, ya puedo mandarle un mensaje como: "Cu√©ntame un chiste acerca de un gato". En este caso, ser√≠a LLaMA 3.2, y me dice: "¬øPor qu√© el gato fue al veterinario? Porque ten√≠a un problema de pelo en la cabeza". No entend√≠ eso.  
   - ![[1.4- Demo Ollama.png]]
   - SLMs populares: LLaMA, Phi (de Microsoft, son muy buenos), DeepSeek R1, Mistral, LLaVA, Gemma, entre otros.

Hemos cubierto un poquito en cuanto a la fundaci√≥n de informaci√≥n. Ahora podemos hablar m√°s acerca de ejemplos utilizando **Python**.

---

![[1.5- Librerias.png]]

## Demos y Ejemplos Pr√°cticos 

[pamelafox/python-openai-demos: A series of short examples using the OpenAI SDK](https://github.com/pamelafox/python-openai-demos). Este repositorio incluye todos los ejemplos pr√°cticos que veremos a continuaci√≥n y est√° dise√±ado para que puedan explorarlo despu√©s de la sesi√≥n a su propio ritmo. Tambi√©n proporcionamos enlaces para configurar un **codespace** a trav√©s del host que prefieran, siendo **GitHub Codespaces** nuestra recomendaci√≥n por ser gratuito y venir preconfigurado. Si optan por usar modelos de **OpenAI**, necesitar√°n una API key. Para **Azure**, ser√° necesario configurar una cuenta, mientras que con **Ollama**, al ser local, no requiere API key. La grabaci√≥n de esta sesi√≥n tambi√©n estar√° disponible en el mismo enlace del repositorio.

---

### Autenticaci√≥n y Configuraci√≥n

La autenticaci√≥n depende del host que elijan para trabajar con los LLMs:  
- **OpenAI (openai.com)**: Requiere una API key que deben configurar como variable de entorno.  
- **Azure OpenAI**: Se recomienda usar **keyless authentication** (autenticaci√≥n sin claves) con **Azure Default Credentials** por seguridad.  
- **Ollama (local)**: Solo necesitan configurar la URL base, sin API key.  
- **GitHub Models**: Fuera de un codespace, necesitan un **Personal Access Token (PAT)**, pero dentro de GitHub Codespaces, todo est√° preconfigurado.

OpenAI demos repo: [pamelafox/python-openai-demos: A series of short examples using the OpenAI SDK](https://github.com/pamelafox/python-openai-demos)
- Para acceder gratis: [aka.ms/python-openai-github](https://aka.ms/python-openai-github)
- Para usar el host de OpenAI: [aka.ms/python-openai-openai](https://aka.ms/python-openai-openai)
- Para utilizar Azure como host: [aka.ms/python-openai-azure](https://aka.ms/python-openai-azure)
- Para conectarte con Ollama: [aka.ms/python-openai-ollama](https://aka.ms/python-openai-ollama)

---

### Ejemplos B√°sicos en Python

A continuaci√≥n, exploraremos algunos ejemplos b√°sicos de c√≥mo interactuar con LLMs usando Python:

1. **Chat Completion Simple**  
   - Abrimos el archivo `chat.py` del repositorio.  
   - Este ejemplo muestra la forma m√°s sencilla de usar un LLM: creamos un **chat completion**.  
   - Configuramos par√°metros como el nombre del modelo (por ejemplo, GPT-4), la **temperatura** (que ajusta la creatividad del modelo), y el n√∫mero de respuestas (`n=1`).  
   - Enviamos una lista de mensajes:  
     - Un mensaje **system**: "Eres un asistente √∫til que hace muchas referencias a gatos y usa emojis üò∫".  
     - Un mensaje **user**: "Escribe un haiku sobre un gato hambriento que quiere at√∫n".  
   - Al ejecutar `python chat.py`, el modelo responde con algo como:  
     ```
     Gato ma√∫lla fuerte,  
     at√∫n en lata lo llama,  
     hambre lo persigue üòø.
     ```

2. **Streaming de Respuestas**  
   - En el archivo `chat_stream.py`, implementamos **streaming** para recibir respuestas token por token, en lugar de esperar el texto completo.  
   - Esto es ideal para aplicaciones en tiempo real donde la rapidez importa.  
   - Configuramos `stream=True` y ajustamos el c√≥digo para procesar los fragmentos de respuesta a medida que llegan.

Estos ejemplos b√°sicos est√°n disponibles en el repositorio para que los prueben y modifiquen.
[pamelafox/python-openai-demos: A series of short examples using the OpenAI SDK](https://github.com/pamelafox/python-openai-demos)

---

# Autenticaci√≥n API para hosts OpenAI

## OpenAI (openai.com)

Para configurar la API key:

```python
client = openai.OpenAI(api_key=os.environ["OPENAI_KEY"])
```

---

## Azure OpenAI (Autenticaci√≥n sin clave)

Utiliza las credenciales predeterminadas de Azure para autenticaci√≥n sin API key:

```python
token_provider = azure.identity.get_bearer_token_provider(
    DefaultAzureCredential(), "https://cognitiveservices.azure.com/.default"
)
client = openai.AzureOpenAI(
    api_version=os.environ["AZURE_OPENAI_VERSION"],
    azure_endpoint=os.environ["AZURE_OPENAI_ENDPOINT"],
    azure_ad_token_provider=token_provider,
)
```

M√°s informaci√≥n: [Keyless Connections](https://learn.microsoft.com/azure/developer/ai/keyless-connections)

---

# Autenticaci√≥n API para OpenAI-like APIs

## Ollama

Configura la base URL a `localhost` y la key a cualquier valor.

## GitHub Models

Configura la base URL al host de GitHub Models y la API key con tu PAT (Personal Access Token):

```python
client = openai.OpenAI(
    base_url="https://models.inference.ai.azure.com",
    api_key=os.environ["GITHUB_TOKEN"]
)
```

En GitHub Codespaces, el `GITHUB_TOKEN` siempre almacenar√° tu PAT.  
Si se ejecuta localmente, crea un nuevo PAT:

```python
client = openai.OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="nokeyneeded",
)
```

---

# Llamada a Chat Completion API

### Ejemplo b√°sico

```python
response = client.chat.completions.create(
    model="gpt-4o",
    temperature=0.7,
    max_tokens=30,
    n=1,
    messages=[
        {"role": "system", "content": "Eres un asistente √∫til que hace muchas referencias a gatos y usa emojis."},
        {"role": "user", "content": "Escribe un haiku sobre un gato hambriento que quiere at√∫n"}
    ]
)
print(response.choices[0].message.content)
```

_Ejemplo completo: `chat.py`_[python-openai-demos/spanish/chat.py at main ¬∑ pamelafox/python-openai-demos](https://github.com/pamelafox/python-openai-demos/blob/main/spanish/chat.py)

### Ejemplo con transmisi√≥n (streaming)

```python
completion = client.chat.completions.create(
    model=MODEL_NAME,
    stream=True,
    messages=[
        {"role": "system", "content": "Eres un asistente √∫til que hace muchas referencias a gatos y usa emojis."},
        {"role": "user", "content": "Escribe un haiku sobre un gato hambriento que quiere at√∫n"}
    ]
)
for event in completion:
    print(event.choices[0].delta.content, end="", flush=True)
```

_Ejemplo completo: `chat_stream.py`_[python-openai-demos/spanish/chat_async.py at main ¬∑ pamelafox/python-openai-demos](https://github.com/pamelafox/python-openai-demos/blob/main/spanish/chat_async.py)


---

### Ventajas y Desventajas de los LLMs

#### **Ventajas**  

- **Creatividad**: Generan contenido original y son ideales para brainstorming o redacci√≥n creativa.  
- **Reconocimiento de patrones**: Excelentes para identificar estructuras en lenguaje y c√≥digo.

#### **Desventajas**  

- **Alucinaciones**: Pueden inventar informaci√≥n con aparente certeza, lo que requiere verificaci√≥n.  
- **Ventana de contexto limitada**: Aunque algunos modelos soportan hasta 128,000 tokens, esto puede no ser suficiente para tareas extensas.  
- **Costo y recursos**: Procesar m√°s tokens aumenta el tiempo, el costo y el consumo energ√©tico.

Conocer estas caracter√≠sticas es clave para aprovechar los LLMs de manera efectiva.

---

### C√≥mo Mejorar los Resultados de los LLMs

Existen varias t√©cnicas para optimizar las respuestas de los LLMs:

1. **Prompt Engineering**  
   - Instrucciones claras y espec√≠ficas sobre tono, formato o estilo mejoran las respuestas.  
   - Ejemplo: "Responde como un pirata üè¥‚Äç‚ò†Ô∏è" cambia el estilo dr√°sticamente.

2. **Few-Shot Examples**  
   - Proporcionar ejemplos gu√≠a al modelo sobre c√≥mo responder.  
   - √ötil para formatos espec√≠ficos sin reentrenamiento.

3. **Llamadas Encadenadas (Chained Calls)**  
   - Usar la salida de una llamada como entrada para la siguiente, refinando resultados paso a paso.

4. **RAG (Recuperaci√≥n Aumentada por Generaci√≥n)**  
   - Combinar generaci√≥n de texto con b√∫squeda en bases de datos. (Se explorar√° el 18).

5. **Llamadas a Funciones y Salidas Estructuradas**  
   - Obtener respuestas en formatos como JSON. (Se cubrir√° el 25).

6. **Fine-Tuning**  
   - Entrenar el modelo con datos espec√≠ficos, aunque es costoso y no siempre necesario.

---

#### Ejemplo de Prompt Engineering

En `prompt_engineering.py`:  [python-openai-demos/spanish/prompt_engineering.py at main ¬∑ pamelafox/python-openai-demos](https://github.com/pamelafox/python-openai-demos/blob/main/spanish/prompt_engineering.py)
- Mensaje de sistema: "Act√∫a como Yoda de Star Wars. Responde con su tono y estilo".  
- Mensaje de usuario: "Expl√≠came qu√© es un gato".  
- Respuesta: "Criatura felina, un gato es. Silencioso y astuto, sus pasos son. Compa√±ero leal, a veces es üò∏".

---

#### Ejemplo de Few-Shot Examples

En `few_shot_examples.py`:  [python-openai-demos/spanish/few_shot_examples.py at main ¬∑ pamelafox/python-openai-demos](https://github.com/pamelafox/python-openai-demos/blob/main/spanish/few_shot_examples.py)
- Mensaje de sistema: "Da pistas en lugar de respuestas completas".  
- Ejemplos:  
  - User: "¬øCapital de Brasil?" ‚Üí Assistant: "Piensa en la ciudad que no es R√≠o".  
- Pregunta: "¬øPlaneta m√°s grande?" ‚Üí Respuesta: "Busca el que tiene una gran mancha roja".

---

#### Ejemplo de Llamadas Encadenadas

En `chain_calls.py`:  [python-openai-demos/spanish/chained_calls.py at main ¬∑ pamelafox/python-openai-demos](https://github.com/pamelafox/python-openai-demos/blob/main/spanish/chained_calls.py)
1. "Explica LLMs en un p√°rrafo".  
2. "Revisa la explicaci√≥n como editor".  
3. "Reescribe el p√°rrafo con los comentarios". 

Resultado: Una explicaci√≥n refinada en varias iteraciones.

---

### Librer√≠as Populares para LLMs

Algunas librer√≠as √∫tiles incluyen:  

- [Langchain - Documentaci√≥n](https://python.langchain.com/)  
    Orquestaci√≥n
    
- [Llamaindex - Documentaci√≥n](https://docs.llamaindex.ai/en/stable/)  
    Orquestaci√≥n para RAG y Agentes
    
- [PydanticAI - Documentaci√≥n](https://ai.pydantic.dev/)  
    Orquestaci√≥n para RAG y Agentes
    
- [Semantic Kernel - Documentaci√≥n]([semantic-kernel ¬∑ PyPI](https://pypi.org/project/semantic-kernel/))  
    Orquestaci√≥n
    
- [Autogen - Documentaci√≥n](https://microsoft.github.io/autogen/)  
    Orquestaci√≥n para flujos de agentes
    
- [Litellm - Documentaci√≥n](https://github.com/BerriAI/litellm)  
    Wrapper para varios hosts
    
- ¬°...y muchos m√°s!
    

El repositorio incluye ejemplos con LangChain, Llama Index y Pandas AI.

[python-openai-demos/spanish/chat_langchain.py at main ¬∑ pamelafox/python-openai-demos](https://github.com/pamelafox/python-openai-demos/blob/main/spanish/chat_langchain.py)
[python-openai-demos/spanish/chat_llamaindex.py at main ¬∑ pamelafox/python-openai-demos](https://github.com/pamelafox/python-openai-demos/blob/main/spanish/chat_llamaindex.py)
[python-openai-demos/spanish/chat_pydanticai.py at main ¬∑ pamelafox/python-openai-demos](https://github.com/pamelafox/python-openai-demos/blob/main/spanish/chat_pydanticai.py)

---

#### C√≥mo Elegir una Librer√≠a

Consideren:  
- **Compatibilidad** con modelos y hosts.  
- **Funcionalidades** como streaming o herramientas.  
- **Facilidad de uso** y depuraci√≥n.  
- **Mantenimiento activo**.  
- **Costo-beneficio** para su proyecto.

---

### Creaci√≥n de Aplicaciones Basadas en LLMs

Para aplicaciones completas:  
- **Frontend**: HTML/JavaScript.  
- **Backend**: Python con **FastAPI**, **Quart**, o **aiohttp** (as√≠ncronos para mayor eficiencia).  
Ejemplo: Una app con chat y visi√≥n para describir im√°genes o usar voz.
[Azure-Samples/openai-chat-vision-quickstart: A demonstration of chatting with uploaded images using OpenAI vision models like gpt-4o.](https://github.com/Azure-Samples/openai-chat-vision-quickstart)

---
![[1.6- APP Arquitectura.png]]

![[1.7- Quart.png]]

blog.pamelafox.org/2023/09/best-practices-for-openai-chat-apps_16.html

![[1.8- Porque Quart.png]]

[Best practices for OpenAI Chat apps: Concurrency](https://blog.pamelafox.org/2023/09/best-practices-for-openai-chat-apps.html)

![[1.9- Porque Quart 2.png]]

# Async Frameworks para Python

Hay varias opciones populares para trabajar de forma as√≠ncrona en Python. Puedes consultar m√°s detalles en este [blog post](https://blog.pamelafox.org/2024/07/should-you-use-quart-or-fastapi-for-ai.html).

## Frameworks y Ejemplo de Apps para Azure AI

- **Quart**  
    Ejemplo de app: [aka.ms/azai/chat](https://aka.ms/azai/chat)  
    Otros ejemplos:
    
    - [aka.ms/chat-vision-app](https://aka.ms/chat-vision-app)
    - [aka.ms/ragchat](https://aka.ms/ragchat)
- **FastAPI**  
    Ejemplo de app: [aka.ms/azai/fastapi](https://aka.ms/azai/fastapi)  
    Otro ejemplo: [aka.ms/rag-postgres](https://aka.ms/rag-postgres)
    
- **Aiohttp**  
    Ejemplo de app: [aka.ms/voicerag/repo](https://aka.ms/voicerag/repo)
    
- **Django con async**  
    (No se proporciona enlace espec√≠fico, pero es una opci√≥n a considerar)
    

---

# Resumiendo en breves palabras todo lo que hemos visto anteriormente

## **1. Funcionamiento de los LLMs**

Los LLMs son modelos de machine learning de gran escala dise√±ados para comprender y generar lenguaje natural. La diapositiva explica su operaci√≥n de forma clara, destacando dos procesos fundamentales:

- **Tokenizaci√≥n**: El texto de entrada (por ejemplo, "Hablaremos de LLMs") se descompone en unidades m√°s peque√±as llamadas tokens. Estos tokens son procesados por el modelo para entender el significado y la estructura del lenguaje.
- **Generaci√≥n de texto**: Los LLMs predicen la siguiente palabra o token bas√°ndose en el contexto previo, expandiendo iterativamente el texto para producir respuestas coherentes. Este proceso se basa en probabilidades calculadas a partir de su entrenamiento masivo con grandes vol√∫menes de datos.

Esta explicaci√≥n desmitifica c√≥mo los LLMs logran entender y responder preguntas, haci√©ndolo accesible incluso para quienes no tienen experiencia previa en IA. Adem√°s, se enfatiza que su capacidad para manejar contexto depende de su tama√±o y del volumen de datos con los que fueron entrenados.

**Puntos clave:**
- La tokenizaci√≥n es el primer paso para que los LLMs procesen lenguaje natural.
- La generaci√≥n iterativa basada en contexto permite respuestas fluidas y relevantes.

---

## **2. Opciones de modelos**

La diapositiva presenta una comparaci√≥n pr√°ctica entre dos tipos principales de LLMs seg√∫n su despliegue:

- **Modelos gestionados (hosted)**: Estos son ofrecidos por plataformas como OpenAI, Azure AI y GitHub Models. Se accede a ellos mediante APIs, lo que elimina la necesidad de infraestructura local. Son ideales para prototipos r√°pidos o experimentaci√≥n, aunque pueden implicar costos asociados. Un punto destacado es que GitHub Models ofrece acceso gratuito, lo cual es una ventaja para desarrolladores con recursos limitados.
- **Modelos locales**: Ejemplificados por herramientas como Ollama, estos modelos se ejecutan en hardware propio, ofreciendo mayor control, privacidad y personalizaci√≥n. Sin embargo, requieren m√°s recursos computacionales, como GPUs potentes, lo que puede ser una barrera para algunos usuarios.

Esta secci√≥n equilibra las ventajas y desventajas de cada enfoque, ayudando a los usuarios a elegir seg√∫n sus necesidades espec√≠ficas, como presupuesto, infraestructura o requisitos de seguridad.

**Puntos clave:**
- Los modelos gestionados son f√°ciles de usar pero pueden generar costos.
- Los modelos locales ofrecen autonom√≠a a cambio de mayores demandas t√©cnicas.

---

## **3. Uso de LLMs desde Python**

Uno de los aspectos m√°s valiosos de la diapositiva es su enfoque pr√°ctico sobre c√≥mo integrar LLMs en Python. Se incluyen ejemplos de c√≥digo que cubren:

- **Autenticaci√≥n**: Var√≠a seg√∫n el proveedor. Por ejemplo, OpenAI requiere una API key, mientras que Azure AI puede usar autenticaci√≥n sin clave en algunos casos. Se recomienda almacenar credenciales en variables de entorno para mantener la seguridad.
- **Interacci√≥n b√°sica**: Se muestra c√≥mo realizar una solicitud simple de "chat completion" para obtener respuestas del modelo.
- **Streaming**: Se explica c√≥mo implementar respuestas en tiempo real, mejorando la experiencia en aplicaciones interactivas.

Los ejemplos son claros y est√°n acompa√±ados de buenas pr√°cticas, como la configuraci√≥n segura de credenciales y el manejo eficiente de respuestas. Esto hace que la secci√≥n sea especialmente √∫til para desarrolladores que buscan implementar LLMs en sus proyectos.

**Puntos clave:**
- La autenticaci√≥n depende del proveedor y debe configurarse cuidadosamente.
- El streaming optimiza la interacci√≥n en aplicaciones en tiempo real.

---

## **4. Mejora de los resultados del LLM**

Dado que las respuestas de los LLMs pueden ser impredecibles o no alinearse perfectamente con las expectativas, la diapositiva aborda t√©cnicas para optimizarlas:

- **Prompt engineering**: Consiste en dise√±ar instrucciones claras y espec√≠ficas para guiar al modelo hacia el tono, estilo o formato deseado. Por ejemplo, un prompt como "Responde como un profesor formal" cambia el estilo de la respuesta.
- **Few-shot examples**: Proporcionar ejemplos dentro del prompt (por ejemplo, "Dado A, responde B") permite al modelo aprender patrones sin necesidad de reentrenamiento.
- **T√©cnicas avanzadas**: Se mencionan conceptos como **llamadas encadenadas** (combinar m√∫ltiples solicitudes) y **RAG** (Recuperaci√≥n Aumentada por Generaci√≥n), aunque se indica que se explorar√°n en detalle m√°s adelante.

Estas estrategias son accesibles y no requieren conocimientos avanzados, lo que las hace ideales para principiantes y expertos por igual. Los ejemplos pr√°cticos refuerzan su utilidad.

**Puntos clave:**
- El prompt engineering es una t√©cnica simple pero efectiva para controlar las respuestas.
- Los few-shot examples ajustan el comportamiento del modelo con m√≠nima intervenci√≥n.

---

## **5. Librer√≠as LLM**

La diapositiva enumera y describe librer√≠as populares de Python para trabajar con LLMs, facilitando su integraci√≥n en proyectos m√°s complejos:

- **LangChain**: Ideal para orquestar flujos de trabajo complejos, como combinar LLMs con bases de datos externas.
- **LlamaIndex**: Enfocada en indexar y recuperar informaci√≥n para aplicaciones basadas en b√∫squeda.
- **Pydantic AI**: Aunque podr√≠a ser un error tipogr√°fico (posiblemente refiri√©ndose a Pandas AI u otra), se asume que ofrece herramientas para estructurar datos en proyectos de IA.

Se proporcionan ejemplos de c√≥digo para conectar estas librer√≠as a modelos como los de GitHub Models, junto con criterios para elegir la m√°s adecuada seg√∫n el caso de uso (por ejemplo, compatibilidad, funcionalidades espec√≠ficas). Esto ayuda a los desarrolladores a tomar decisiones informadas.

**Puntos clave:**
- Cada librer√≠a tiene fortalezas √∫nicas, desde orquestaci√≥n hasta integraci√≥n de datos.
- La elecci√≥n depende de los objetivos y la complejidad del proyecto.

---

## **6. Creaci√≥n de aplicaciones basadas en LLMs**

La diapositiva culmina con un ejemplo pr√°ctico: una aplicaci√≥n de chat con capacidades de visi√≥n. Este caso demuestra c√≥mo los LLMs pueden combinarse con otras tecnolog√≠as (como el procesamiento de im√°genes) para crear soluciones innovadoras. Algunos detalles incluyen:

- **Framework**: Se usa Quart, un framework as√≠ncrono, para manejar m√∫ltiples solicitudes eficientemente.
- **Arquitectura**: Se ilustra de forma sencilla, mostrando la interacci√≥n entre el frontend, el backend y el modelo.
- **Recursos**: Se proporcionan enlaces a ejemplos adicionales, incentivando la experimentaci√≥n.

Este enfoque pr√°ctico inspira a los desarrolladores a ir m√°s all√° de las interacciones b√°sicas y explorar aplicaciones escalables y multifuncionales.

**Puntos clave:**
- Los LLMs pueden integrarse con visi√≥n y otros tipos de datos para aplicaciones avanzadas.
- Los frameworks as√≠ncronos mejoran el rendimiento en entornos de alta demanda.

---




[[Gu√≠a r√°pida de fundamentos en Python]]
[[2- Laboratorios de Python]]
[[1- Qu√© son y c√≥mo Declarar Variables]]
[[2- Las Listas]]
[[3- Las Tuplas]]
[[4- Entrada de Informaci√≥n por parte del Usuario]]
[[5- Entrada de Informaci√≥n por parte del Usuario (Argumentos)]]
[[6- Los Diccionarios]]
[[7- Los Operadores L√≥gicos]]
[[8- Sentencias Condicionales]]
[[9- Bucle FOR]]
[[10- Bucle WHILE]]
[[11- Las Funciones]]
