
Gwyneth Pe√±a-Siguenza 
Python Cloud Advocate
madebygps.com/about 
@madebygps
Alias (GPS) üòä
## Sesi√≥n Completa sobre Embeddings Vectoriales: Detalle Paso a Paso

Buenas tardes a todos y todas. Bienvenidos a esta segunda sesi√≥n de nuestra serie sobre Python e Inteligencia Artificial, organizada por Microsoft. Mi nombre es [su nombre, si aplica], y hoy, 13 de marzo, vamos a sumergirnos en un tema esencial y apasionante en el campo de la inteligencia artificial: **los embeddings vectoriales**. Este concepto es un pilar fundamental para muchas aplicaciones de IA, y estoy emocionado de compartirlo con ustedes. Antes de empezar, quiero asegurarme de que todo est√© en orden: ¬øme escuchan bien? ¬øPueden ver mi pantalla sin problemas? Por favor, confirmen en el chat para que podamos proceder con tranquilidad.

Esta sesi√≥n forma parte de una serie m√°s amplia que incluye temas como Modelos de Lenguaje de Gran Escala (LLMs) el 11 de marzo, Recuperaci√≥n Aumentada con Generaci√≥n (RAG) el 18 de marzo, modelos de visi√≥n el 19 de marzo, salidas estructuradas el 25 de marzo y calidad y seguridad el 27 de marzo. Si a√∫n no se han inscrito, pueden hacerlo en el enlace [aka.ms/PythonIA/series](https://aka.ms/PythonIA/series). Adem√°s, todas las diapositivas de esta presentaci√≥n est√°n disponibles para descargar en [aka.ms/pythonia/embeddings/diapositivas](https://aka.ms/pythonia/embeddings/diapositivas). Les recomiendo tenerlas a la mano para seguir el contenido paso a paso.

### Agenda de la Sesi√≥n

Hoy cubriremos los siguientes temas, cada uno explicado en detalle:

1. **¬øQu√© son los embeddings vectoriales?**
2. **Espacio de similitud vectorial**
3. **B√∫squeda vectorial**
4. **M√©tricas de distancia vectorial**
5. **Cuantizaci√≥n vectorial**
6. **Reducci√≥n de dimensionalidad**

Sin m√°s pre√°mbulos, empecemos con el primer punto.

---

### 1. ¬øQu√© son los Embeddings Vectoriales?

Comencemos por definir qu√© son los **embeddings vectoriales**. En t√©rminos simples, un embedding vectorial es una representaci√≥n num√©rica de datos, generalmente texto, en forma de una lista de n√∫meros de punto flotante. Imaginen que queremos que una m√°quina entienda el lenguaje humano: palabras como "perro", "gato" o frases como "hola mundo". Las computadoras no pueden procesar palabras directamente, as√≠ que las convertimos en vectores, que son listas de n√∫meros que representan esos datos en un espacio matem√°tico multidimensional.

Por ejemplo, la palabra "perro" podr√≠a representarse como:

```
[0.017198, -0.007493, -0.057982, ‚Ä¶]
```

Este vector no es aleatorio; cada n√∫mero captura alg√∫n aspecto del significado o contexto de "perro", aprendido por un modelo de inteligencia artificial durante su entrenamiento. La longitud de estos vectores, conocida como su **dimensionalidad**, depende del modelo que los genera. Aqu√≠ tienen algunos ejemplos concretos de modelos populares y sus caracter√≠sticas:

| **Modelo**                | **Qu√© codifica**          | **Longitud del vector (Dimensiones)** |
|---------------------------|---------------------------|---------------------------------------|
| Word2Vec                 | Palabras individuales     | 300                                   |
| SBERT                    | Texto (~400 palabras)     | 768                                   |
| OpenAI text-embedding-ada-002 | Texto (hasta 8191 tokens) | 1536                            |
| OpenAI text-embedding-3-small | Texto (hasta 8191 tokens) | 512 o 1536                      |
| OpenAI text-embedding-3-large | Texto (hasta 8191 tokens) | 256, 1024 o 3072                |

Cada modelo tiene un prop√≥sito. Por ejemplo, Word2Vec es excelente para palabras individuales, mientras que `text-embedding-3-large` de OpenAI, con hasta 3072 dimensiones, captura matices sem√°nticos m√°s complejos en textos largos. La dimensionalidad afecta tanto la precisi√≥n como el costo computacional: m√°s dimensiones suelen significar mayor detalle, pero tambi√©n m√°s recursos.

#### ¬øC√≥mo se generan los embeddings?

Estos vectores no los inventamos nosotros; los genera un modelo de embeddings previamente entrenado. Vamos a ver c√≥mo hacerlo en la pr√°ctica usando el SDK de OpenAI. Aqu√≠ tienen un ejemplo de c√≥digo en Python para generar un embedding:

```python
import openai
import os

# Configuramos el cliente de OpenAI
openai_client = openai.OpenAI(
    base_url="https://models.inference.ai.azure.com",  # Endpoint de la API
    api_key=os.environ["GITHUB_TOKEN"]                # Clave de autenticaci√≥n
)

# Solicitamos el embedding
response = openai_client.embeddings.create(
    model="text-embedding-3-small",  # Modelo elegido
    input="hola mundo",              # Texto a codificar
    dimensions=1536                  # Dimensionalidad deseada
)

# Imprimimos el vector resultante
print(response.data[0].embedding)
```

Paso a paso, este c√≥digo:
1. Importa las bibliotecas necesarias (`openai` y `os`).
2. Crea un cliente de OpenAI con una URL base y una clave de API (en este caso, un token de GitHub almacenado como variable de entorno).
3. Llama al m√©todo `embeddings.create` con tres par√°metros:
   - `model`: Especifica el modelo, aqu√≠ `text-embedding-3-small`.
   - `input`: El texto que queremos convertir en vector, "hola mundo".
   - `dimensions`: La longitud del vector, 1536 en este caso.
4. El resultado, almacenado en `response.data[0].embedding`, es una lista de 1536 n√∫meros que representa "hola mundo".

Si quieren probar esto, pueden usar el notebook `generate_embedding.ipynb` disponible en el repositorio de la sesi√≥n. El vector resultante ser√° algo como:

```
[0.013245, -0.009876, 0.045632, ‚Ä¶]  # 1536 valores
```

#### Diferencias entre modelos

No todos los modelos producen el mismo embedding para una palabra. Por ejemplo, la palabra "queen" tendr√° vectores distintos en `word2vec` (300 dimensiones), `text-embedding-ada-002` (1536 dimensiones) o `text-embedding-3-small` (512 o 1536 dimensiones). Esto se debe a que cada modelo fue entrenado con datos y t√©cnicas diferentes, lo que afecta c√≥mo "interpreta" el significado. Estas diferencias son cruciales cuando medimos similitudes, como veremos m√°s adelante.

---

### 2. Espacio de Similitud Vectorial

Ahora que sabemos qu√© son los embeddings, pasemos al siguiente concepto: el **espacio de similitud vectorial**. Los embeddings no solo representan datos, sino que tambi√©n nos permiten comparar cu√°n similares son entre s√≠. Imaginen un espacio tridimensional (aunque en realidad tiene cientos o miles de dimensiones): cada vector es un punto, y la distancia entre puntos refleja la similitud sem√°ntica.

#### Similitud del coseno

La m√©trica m√°s com√∫n para medir esta similitud es la **similitud del coseno**, que calcula el coseno del √°ngulo entre dos vectores. Aqu√≠ est√° la f√≥rmula matem√°tica:

```math
\text{Similitud del coseno} = \frac{\mathbf{v1} \cdot \mathbf{v2}}{\|\mathbf{v1}\| \|\mathbf{v2}\|}
```

Desglosemos esto:
- **`v1 ¬∑ v2`**: El producto punto, que es la suma de los productos de cada par de componentes de los vectores.
- **`‚Äñv1‚Äñ` y `‚Äñv2‚Äñ`**: Las magnitudes (o normas) de los vectores, calculadas como la ra√≠z cuadrada de la suma de los cuadrados de sus componentes.
- El resultado est√° entre -1 y 1:
  - **1**: Los vectores son id√©nticos en direcci√≥n (m√°xima similitud).
  - **0**: Son perpendiculares (sin similitud).
  - **-1**: Apuntan en direcciones opuestas (m√°xima disimilitud).

Implementemos esto en Python:

```python
def cosine_similarity(v1, v2):
    # Calculamos el producto punto
    dot_product = sum(a * b for a, b in zip(v1, v2))
    # Calculamos las magnitudes
    magnitude_v1 = (sum(a**2 for a in v1)) ** 0.5
    magnitude_v2 = (sum(b**2 for b in v2)) ** 0.5
    # Devolvemos la similitud
    return dot_product / (magnitude_v1 * magnitude_v2)

# Ejemplo con vectores peque√±os
v1 = [1, 2, 3]
v2 = [2, 4, 6]
print(cosine_similarity(v1, v2))  # Resultado: 1.0 (vectores proporcionales)
```

En la pr√°ctica, con embeddings reales, podr√≠amos comparar "perro" y "gato" y obtener una similitud de, digamos, 0.56, mientras que "perro" y "avena" podr√≠an dar 0.12, reflejando que "perro" y "gato" son m√°s similares sem√°nticamente.

#### Variaci√≥n entre modelos

El rango de similitudes depende del modelo. Por ejemplo:
- En `text-embedding-ada-002`, las similitudes suelen estar entre 0.7 y 0.9.
- En `text-embedding-3-large`, el rango es m√°s amplio, como 0.1 a 0.5.

Esto ocurre porque cada modelo distribuye los vectores de manera diferente en el espacio, influenciado por su entrenamiento y dimensionalidad.

#### Ejemplo pr√°ctico: Recomendaciones

Pensemos en un sistema de recomendaci√≥n de recetas. Si buscamos recetas similares a "Apple Pie", podr√≠amos obtener:

| **Receta**                | **Similitud del coseno** |
|---------------------------|--------------------------|
| Apple Pie by Grandma Ople | 0.000000                |
| Easy Apple Pie            | 0.051372                |
| Grandma's Iron Skillet Apple Pie | 0.054287         |

Aqu√≠, "Apple Pie by Grandma Ople" tiene similitud 0 consigo misma (la consulta), y las otras recetas son cercanas en significado. Esto muestra c√≥mo la similitud vectorial puede impulsar recomendaciones relevantes.

---

### 3. B√∫squeda Vectorial

Pasemos ahora a la **b√∫squeda vectorial**, una t√©cnica que usa embeddings para encontrar los elementos m√°s similares a una consulta en una base de datos. Esto es clave en aplicaciones como motores de b√∫squeda sem√°ntica o sistemas de recomendaci√≥n.

#### Proceso b√°sico

Supongamos que tenemos una base de datos con embeddings de palabras como "culebra", "serpiente" y "lagarto". El proceso es:
1. Generamos el embedding de la consulta, por ejemplo, "culebra".
2. Comparamos este vector con todos los vectores en la base de datos.
3. Devolvemos los `k` m√°s similares (e.g., los 3 m√°s cercanos).

Hay dos enfoques principales:

##### B√∫squeda Exhaustiva

Compara la consulta con cada vector en la base de datos. Aqu√≠ un ejemplo en Python:

```python
def exhaustive_search(query_vector, vectors):
    # Calculamos similitudes para cada vector
    similarities = [(title, cosine_similarity(query_vector, vector)) 
                    for title, vector in vectors.items()]
    # Ordenamos de mayor a menor similitud
    similarities.sort(key=lambda x: x[1], reverse=True)
    # Devolvemos la lista ordenada
    return similarities

# Ejemplo
vectors = {
    "culebra": [0.1, 0.2, 0.3],
    "serpiente": [0.12, 0.19, 0.31],
    "lagarto": [0.5, 0.6, 0.7]
}
query = [0.1, 0.2, 0.3]  # Embedding de "culebra"
results = exhaustive_search(query, vectors)
print(results)  # [("culebra", 1.0), ("serpiente", 0.998), ("lagarto", 0.974)]
```

Esto funciona bien con pocos datos, pero con millones de vectores, es demasiado lento.

##### B√∫squeda Aproximada (ANN)

Para bases de datos grandes, usamos algoritmos de **b√∫squeda aproximada de vecinos m√°s cercanos (ANN)**, como HNSW (Hierarchical Navigable Small Worlds). Aqu√≠ un ejemplo con la librer√≠a `hnswlib`:

```python
import hnswlib
import numpy as np

# Datos de ejemplo
vectors = np.array([[0.1, 0.2, 0.3], [0.12, 0.19, 0.31], [0.5, 0.6, 0.7]])
ids = np.arange(len(vectors))

# Creamos el √≠ndice
p = hnswlib.Index(space='cosine', dim=3)  # Espacio de coseno, 3 dimensiones
p.init_index(max_elements=1000, ef_construction=200, M=16)  # Configuraci√≥n
p.add_items(vectors, ids)  # A√±adimos vectores
p.set_ef(50)  # Par√°metro de b√∫squeda

# Buscamos
query = np.array([0.1, 0.2, 0.3])
labels, distances = p.knn_query(query, k=2)  # 2 vecinos m√°s cercanos
print(labels, distances)  # Ejemplo: [0, 1], [0.0, 0.002]
```

HNSW es r√°pido y eficiente, con un crecimiento logar√≠tmico en tiempo de b√∫squeda, ideal para √≠ndices din√°micos.

#### Algoritmos ANN populares

| **Algoritmo** | **Librer√≠a Python** | **Bases de Datos Soportadas**                  |
|---------------|---------------------|-----------------------------------------------|
| HNSW          | hnswlib             | PostgreSQL (pgvector), Azure AI Search, ChromaDB, Weaviate |
| DiskANN       | diskannpy           | Cosmos DB                                     |
| IVFFlat       | faiss               | PostgreSQL (pgvector)                         |
| Faiss         | faiss               | Solo √≠ndice en memoria                        |

Cada uno tiene ventajas: HNSW es vers√°til, mientras que DiskANN optimiza almacenamiento en disco.

#### Aplicaciones

Un caso pr√°ctico es la **Recuperaci√≥n Aumentada con Generaci√≥n (RAG)**, donde la b√∫squeda vectorial encuentra documentos relevantes para enriquecer respuestas de modelos de lenguaje. Otro ejemplo es recomendar productos o detectar fraudes comparando patrones.

---

### 4. M√©tricas de Distancia Vectorial

Para comparar vectores, usamos m√©tricas de distancia o similitud. Revisemos las principales:

1. **Distancia Euclidiana**: La distancia en l√≠nea recta entre dos puntos.
   ```math
   \text{Distancia Euclidiana} = \sqrt{\sum_{i=1}^n (x_i - y_i)^2}
   ```
   Ejemplo: Entre `[1, 2]` y `[4, 6]`, es `‚àö((1-4)¬≤ + (2-6)¬≤) = ‚àö25 = 5`.

2. **Distancia de Manhattan**: Suma de diferencias absolutas.
   ```math
   \text{Distancia de Manhattan} = \sum_{i=1}^n |x_i - y_i|
   ```
   Ejemplo: Entre `[1, 2]` y `[4, 6]`, es `|1-4| + |2-6| = 3 + 4 = 7`.

3. **Producto Punto**: Suma de productos de componentes.
   ```math
   \text{Producto Punto} = \sum_{i=1}^n x_i y_i
   ```
   Ejemplo: Entre `[1, 2]` y `[4, 6]`, es `1*4 + 2*6 = 4 + 12 = 16`.

4. **Distancia del Coseno**: Complemento de la similitud del coseno.
   ```math
   \text{Distancia del Coseno} = 1 - \text{Similitud del Coseno}
   ```

#### Vectores unitarios

Un vector unitario tiene magnitud 1 (normalizado). En estos casos, el producto punto equivale a la similitud del coseno, simplificando c√°lculos. Los modelos de OpenAI generan vectores unitarios por defecto.

---

### 5. Cuantizaci√≥n Vectorial

La **cuantizaci√≥n vectorial** reduce el tama√±o de los embeddings para ahorrar espacio y acelerar b√∫squedas, sacrificando algo de precisi√≥n.

#### M√©todos

- **Cuantizaci√≥n Escalar**: Convierte floats (32 bits) a enteros (8 bits).
  - Proceso:
    1. Normalizar valores al rango [0, 1].
    2. Mapear a [-128, 127].
  - Ejemplo: `[0.032651, 0.013703]` ‚Üí `[53, 40]`.

- **Cuantizaci√≥n Binaria**: Reduce a 1 bit (0 o 1).
  - Proceso: 1 si el valor es mayor al promedio, 0 si es menor.
  - Ejemplo: `[0.032651, 0.013703, -0.01]` ‚Üí `[1, 1, 0]`.

#### Efectos

- **Similitud**: Con int8, los resultados son casi id√©nticos a float32; con binario, hay m√°s variaci√≥n, pero sigue siendo √∫til.
- **Almacenamiento**: Un √≠ndice de 1177 MB (float32) baja a 298 MB (int8) o 41 MB (binario) en Azure AI Search.

Prueben el notebook `quantization.ipynb` para verlo en acci√≥n.

---

### 6. Reducci√≥n de Dimensionalidad

La **reducci√≥n de dimensionalidad** disminuye la longitud de los vectores, preservando informaci√≥n clave. Una t√©cnica moderna es **Matryoshka Representation Learning (MRL)**, soportada por `text-embedding-3-large`.

#### Ejemplo con OpenAI SDK

```python
response = openai_client.embeddings.create(
    model="text-embedding-3-small",
    input="hello world",
    dimensions=256  # Reducimos de 1536 a 256
)
```

#### Efectos

Comparando "Moana":

| **Pel√≠cula**          | **Similitud (1536 dim)** | **Similitud (256 dim)** |
|-----------------------|--------------------------|-------------------------|
| Moana                | 1.0000                  | 1.0000                 |
| Mulan                | 0.5468                  | 0.5834                 |
| Lilo & Stitch        | 0.5021                  | 0.5760                 |

La similitud se mantiene razonable con menos dimensiones.

#### Combinaci√≥n con cuantizaci√≥n

Para m√°xima eficiencia:
- Reduzca dimensiones (e.g., a 256).
- Cuantice a int8 o binario.
- Sobremuestree resultados y reeval√∫e con vectores originales.

---

### Conclusi√≥n

Hemos recorrido los embeddings vectoriales desde su definici√≥n hasta t√©cnicas avanzadas. Estos conceptos son fundamentales para aplicaciones como RAG, que exploraremos el 18 de marzo. Revisen los notebooks (`similarity.ipynb`, `search.ipynb`, etc.) y √∫nanse a la pr√≥xima sesi√≥n. ¬°Gracias y nos vemos pronto!

--- 

